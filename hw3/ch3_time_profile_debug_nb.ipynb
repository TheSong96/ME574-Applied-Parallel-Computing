{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36564bitconda16f9fed6d36b48b1bbd0e562616a7c7a",
   "display_name": "Python 3.6.5 64-bit (conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 3 Tools for Timing, Profiling, and Debugging\n",
    "\n",
    "Having created both serial and parallel versions of the map app in\n",
    "Chapter 2 \"Function Evaluations and the Map Pattern\" a natural next\n",
    "step is to compare the execution times to quantify the \"acceleration\"\n",
    "factor. In this chapter, we introduce some basic tools for timing, profiling,\n",
    "and debugging.\n",
    "\n",
    "## 3.1 Timing Comparisons\n",
    "\n",
    "### 3.1.1 Simple Python Timing\n",
    "\n",
    "Let's start simply with a standard python library function for obtaining \"wall clock\" times. The actual function is called `time` and it resides in a package also called `time`, so to avoid code like `time.time()`, we'll do the import as `from time import time`. The basic idea is to read the clock before and after the code block of interest and to obtain the runtime as the difference of the clock readings. The listing of _map_main_timed.py_ below shows a version of _map_main.py_ that has been modified to execute the serial version of `sArray` and then to execute the parallel version of `sArray` twice. A few lines of code is added to obtain and print the runtime for each as measured using `time`.\n",
    "\n",
    "```\n",
    "File: main_timed.py\n",
    "01: import numpy as np\n",
    "02: import matplotlib.pyplot as plt\n",
    "03: from time import time #import timing function\n",
    "04: from numba import cuda\n",
    "05: N = 640000\n",
    "06: \n",
    "07: def main():\n",
    "08:     start_all = time() #start overall timer\n",
    "09:     x = np.linspace(0, 1, N, endpoint=True)\n",
    "10:     from serial import sArray\n",
    "11:     start = time() #start timer for serial execution\n",
    "12:     f = sArray(x)\n",
    "13: \tend = time() #stop timer for serial execution\n",
    "14:     elapsed_serial = end - start #compute serial runtime\n",
    "15:     print(\"--- Serial timing: %0.4f seconds ---\" % elapsed_serial)\n",
    "16: \n",
    "17:     from parallel import sArray #import parallel version of sArray\n",
    "18:     \n",
    "19:     for i in range(2):\n",
    "20:         start = time() #start timer for parallel execution\n",
    "21:         fpar = sArray(x)\n",
    "22:         end = time() #stop timer for parallel execution\n",
    "23:         elapsed = end - start #compute parallel runtime\n",
    "24:         print(\"--- Parallel timing #%d: %3.4f seconds ---\" % (i,elapsed))\n",
    "25: \n",
    "26:     print(\"--- Loop acceleration estimate: %dx ---\" % (elapsed_serial//elapsed))\n",
    "27:     end_all = time() #end overall timer\n",
    "28:     elapsed = end_all - start_all #evaluate overall runtime\n",
    "29:     print(\"--- Total time: %3.4f seconds ---\" % elapsed)\n",
    "30:     print(\"--- Total acceleration estimate: %3.4fx ---\" % ((3*elapsed_serial)/(elapsed)))\n",
    "31: \n",
    "32: if __name__ == '__main__':\n",
    "33:     main()\n",
    "\n",
    "```\n",
    " $$ \\text{Listing 3.1: } map\\_timed.py$$\n",
    "\n",
    "In the code above, 2 sets of timing variables are defined:\n",
    "\n",
    "- `start_all` and `end_all` are used to record the times bounding execution of the entire code. On line 8, `start_all = time()` records the time as `main()` begins execution; on line 27, `end_all = time()` records the time when all the calls to `sArray` have completed. The overall runtime is computed by `elapsed = end_all - start_all` on line 28, and line 30 prints the result to the terminal. \n",
    "\n",
    "- `start` and `end` are used to record the times bounding specific executions of `sArray`.\n",
    "\n",
    "  - The statements `start = time()` on line 11 and `end = time()` on line 13 bracket the call to the serial implementation of `sArray`. Lines 14-15 compute the serial runtime (`elapsed_serial = end - start`) and print the result to the terminal.\n",
    "\n",
    "  - The statements `start = time()` on line 20 and `end = time()` on line 22 bracket the calls to the parallel implementation of `sArray`. Lines 23-24 compute the parallel runtimes (`elapsed = end - start`) and print the results to the terminal.\n",
    "\n",
    "Let's examine a couple example outputs from running _map_main_timed.py_ for different array sizes. \n",
    "\n",
    "```\n",
    "--- Serial timing: 0.4842 seconds ---\n",
    "--- Parallel timing #0: 1.4666 seconds ---\n",
    "--- Parallel timing #1: 0.0060 seconds ---\n",
    "--- Loop acceleration estimate: 80x ---\n",
    "--- Total time: 1.9746 seconds ---\n",
    "--- Total acceleration estimate: 0.7357x ---\n",
    "```\n",
    "The output above is for `N=640000`, and there are several salient features worthy of discussion. The timing estimate for the serial execution is about 0.5s; the timings for the 2 parallel evaluations are quite different: one longer than the serial timing and one much shorter. Why are they different? When this code was executed, the decorator preceding the kernel definition in _map_main.parallel.py_ was simply `@cuda.jit()`; i.e. without the optional signature specification. As a result, \"lazy compilation\" occurs and the kernel code is compiled \"just-in-time\" at the first call for execution. The timing for the first parallel evaluation is longer because it includes the time for kernel compilation.\n",
    "\n",
    "> When measuring runtimes of kernel functions, always measure more than once and remember that the timing for the first execution may be extremely \"pessimistic\" due to inclusion of compilation time.\n",
    "\n",
    "Given the timings for 2 parallel executions, we estimate that the 1.5s timing is almost entirely spent on compilation, and the actual time for kernel execution is about 6ms. The \"acceleration factor\", the ratio of execution time for serial ``sArray` to execution time for parallel `sArray`, is estimated as $\\frac{0.4842}{0.0060} = 80 \\times$.\n",
    "\n",
    "> To engineers, this is an imprecise use of \"acceleration\" (\"speedup\" might be better), but it is already an established way to refer to the runtime ratio (followed by '$\\times$', sometimes read as 'times'). \n",
    "\n",
    "Finally, contrast the acceleration factors for sArray itself (80x) and for the entire code which is estimated as the time for the entire code (which executes `sArray` 3 times) with the time for 3 serial executions. In this case, the overall acceleration factor is $0.73 \\times$, so parallelization appears to make the computation take longer! However, as mentioned above, we need to take into account the fact that one of the parallel timings includes compilation time that, by itself, takes as much time as the 3 serial executions.\n",
    "\n",
    "So if we are careful to run the timing twice, we see that parallelization reduces runtime of `sArray` by $80 \\times$ which appears quite worthwhile. But is there really anything special about our chosen value of `N`? Perhaps the better question is how the runtime scales with parameters like problem size and processor count. At this point, it would not be productive to get sidetracked doing an in-depth study, but let's look at one more case before moving on. In particular, let's increase the array size by an order of magnitude (`N = 64000000`) and modify the kernel decorator to include a signature. With those 2 changes, the following results are produced:\n",
    "\n",
    "```\n",
    "--- Serial timing: 4.9607 seconds ---\n",
    "--- Parallel timing #0: 0.0343 seconds ---\n",
    "--- Parallel timing #1: 0.0398 seconds ---\n",
    "--- Loop acceleration estimate: 124x ---\n",
    "--- Total time: 6.6667 seconds ---\n",
    "--- Total acceleration estimate: 2.2323x ---\n",
    "```\n",
    "\n",
    "Now the serial version of `sArray` takes almost 5s which, not surprisingly (since there are 10 times as many entries to compute), is almost exactly 10 times as long as the previous serial timing. Moving on to the parallel timings, we see that (with a signature specification allowing \"eager compilation\" to occur before execution) there is no longer a major difference between the parallel runtimes, and the loop acceleration factor is still signficant ($\\approx 125 \\times$).\n",
    "\n",
    "Finally, let's consider the \"total acceleration estimate\". The factor is now greater than 1, but a big question remains:\n",
    "\n",
    "__Why must the total acceleration factor be so much less than the loop acceleration factor?__\n",
    "\n",
    "One might hope that as more processors are used, the runtime will continue decreasing and the acceleration factor will continue increasing. This may actually be the case, but there are limits that must be taken into account. In the current case, the complete code includes execution of the serial version of `sArray`, so there is a portion of the overall task that can be parallelized and there is a portion that is inherently serial. (Here, we insisted on an execution of serial `sArray` for timing comparison but, more typically, there is just some portion of the task that is not appropriate for or amenable to parallelization.) A typical terminology uses $p$ to denote the fraction of the overall task that is amenable to parallelization (so the remaining non-parallelizable fraction is $1-p$) and $s$ the achievable parallel acceleration factor. Given those quantities, __Amdahls' law__ gives the acceleration factor for the entire code, $S_{latency}$ as:\n",
    "\n",
    "$$ S_{latency} = \\frac{1}{1-p + (p/s)}$$\n",
    "\n",
    "Even when approaching the case of unlimited resources (e.g. infinitely many cores) where $s \\rightarrow \\infty$ and $p/s \\rightarrow 0$, Amdahl's law sets an upper bound on the acceleration or __latency reduction factor__:\n",
    "\n",
    "$$ S_{latency} < \\frac{1}{1-p}$$\n",
    "\n",
    "To be concrete, we are considering an example involving 3 calls to execute `sArray`, one of which must be a serial execution. Thus the parallelizable fraction of the task is $2/3$ and the overall acceleration is limited by \n",
    "\n",
    "$$ S_{latency} < \\frac{1}{1-(2/3)} = \\frac{1}{(1/3)} = 3$$\n",
    "\n",
    "In the second set of results (with `N = 6400000` and eager compilation), the overall acceleration is already $2.23$ and all the additional processors in the world can only provide further reduction of execution time by about $\\frac{(1/2.23)-(1/3)}{(1/3)} = 0.25$ or $25\\%$.\n",
    "\n",
    "> Bottom line: Additional parallel resources are helpful, but there are some hard limits and expectatons should be adjusted to avoid \"unbridled enthusiasm\".\n",
    "\n",
    "Suppose you want to time the execution of a CUDA kernel. What happens if we apply `time()` in that situation? Let's try it and find out.\n",
    "\n",
    "Here we return to running _map_main_ (with the plotting statements removed since they are not really of interest at the moment) and call a modified version of _map_parallel.py_ (let's call it _map_parallel_timed.py_) that includes `time()` calls wrapped around the array transfer to the device and the kernel execution. A listing of _map_parallel_timed.py_ is shown below:\n",
    "\n",
    "```\n",
    "File: parallel_time.py\n",
    "01: import math\n",
    "02: import numpy as np\n",
    "03: from numba import jit, cuda, float32\n",
    "04: from time import time\n",
    "05: \n",
    "06: PI = np.pi\n",
    "07: TPB = 32\n",
    "08: \n",
    "09: @cuda.jit(device = True)\n",
    "10: def s(x0):\n",
    "11: \treturn (1.-2.*math.sin(PI*x0)**2)\n",
    "12: \n",
    "13: @cuda.jit #Lazy compilation\n",
    "14: #@cuda.jit('void(float32[:], float32[:])') #Eager compilation\n",
    "15: def sKernel(d_f, d_x):\n",
    "16: \ti = cuda.grid(1)\n",
    "17: \tn = d_x.shape[0]\t\n",
    "18: \tif i < n:\n",
    "19: \t\td_f[i] = s(d_x[i]) #content of `for` loop in serial version\n",
    "20: \n",
    "21: def sArray(x):\n",
    "22: \tn = x.shape[0]\n",
    "23: \td_x = cuda.to_device(x)\n",
    "24: \td_f = cuda.device_array(n, dtype = np.float32) #need dtype spec for eager compilation\n",
    "25: \tblockDims = TPB\n",
    "26: \tgridDims = (n+TPB-1)//TPB\n",
    "27: \n",
    "28: \tstart = time()\n",
    "29: \tsKernel[gridDims, blockDims](d_f, d_x)\n",
    "30: \tend = time()\n",
    "31: \telapsed_time = end -  start\n",
    "32: \tprint(\"--- Kernel time(): %3.4f milliseconds ---\" % (1000*elapsed_time))\n",
    "33: \treturn d_f.copy_to_host()\n",
    "```\n",
    "\n",
    "$$ \\text{Listing 3.2 - } map\\_parallel\\_time.py$$\n",
    "\n",
    "Here we see `time()` statements wrapped around the kernel call on line 29. Note that the units have shifted to milliseconds, so results from `time()` (which are in seconds) get multiplied by $1000$ in the print statements on lines 32. Executing _map.py_ (with `sArray` imported from this modified file) produces the following results:\n",
    "\n",
    "```\n",
    "--- Kernel time(): 402.4272 milliseconds ---\n",
    "--- Kernel time(): 0.0000 milliseconds ---\n",
    "```\n",
    "\n",
    "The fact that the second kernel appears to take no time at all is, at the very least suspicious. If we change the decorator preceding the kernel to include a signature and enable ahead-of-time compilation, then the following result is obtained:\n",
    "\n",
    "```\n",
    "--- Kernel time(): 370.4424 milliseconds ---\n",
    "--- Kernel time(): 0.0000 milliseconds ---\n",
    "```\n",
    "\n",
    "Now both timings are $0$ ms, abd we can deduce that the first (non-zero) reported time interval is related to lazy compilation at runtime. The timing result above is clearly not a valid timing for kernel execution, and this brings up an important topic.\n",
    "\n",
    "### 3.1.2 Synchronous vs. Asynchronous Execution\n",
    "\n",
    "For almost all of us, our computing experince is firmly CPU-based, and we typically think of function calls executing sequentially: the first function is called, starts running, and completes execution; then the next function can start and, when it has completed, the next function can begin; etc.\n",
    "\n",
    "Upon entering the parallel world computing, we need to think about things differently. The order in which functions are called does not govern the order of execution. As mentioned previously, we give up control over the order of execution in return for the throughput enhancements offered by parallel execution. To take advantage of parallelism, whenever possible we allow processors to proceed with computations without waiting for results from other processors. In the sequential model of CPU-based serial computing, __synchronous execution__ (where we call a function and, after it has completed execution, the next function can begin execution) is the standard. In contrast, once we go parallel, that assumption gets flipped completely. In particular, when a kernel is called from the host (CPU) to run on the device (GPU), the moment the kernel is launched to start executing on the GPU the CPU moves on to its next task. This is the __asynchronous execution__ model. Waiting around for execution of the kernel would needlessly cause the CPU to be idle and, in pursuit of efficiency, the CPU continues with whatever computing task it can perform instead of sitting idle.\n",
    "\n",
    "The standard python timing tools are generally sufficient for timing non-trivial (taking longer than the ~1 ms resolution of `time()`) synchronous execution. It turns out that `cuda.to_device()` can be either synchronous or asynchronous (refer to the numba docs for details), but kernel execution is asynchronous. The result from `time()` only measures the time to _launch the kernel_ (to start the execution), not the time to _execute the kernel_ which is what we really want to measure. For that we need a different set of tools.\n",
    "\n",
    "### 3.1.3 CUDA Event Timing\n",
    "\n",
    "To reliably time CUDA operations (which may have sub-millisecond duration and be asynchronous), CUDA provides a timing model based on __events__. The basic usage of events involves creating events, recording times, computing time intervals, and synchronization (to ensure full timing of asynchronous execution.) Let's stick to our terminology of using variables `start` and `end` to store clock readings that bound the interval to be measured. To be concrete, consider a snippet using `time()` to  determine the runtime for `sKernel`:\n",
    "\n",
    "```\n",
    "start = time()\n",
    "sKernel[gridDims, blockDims](d_f, d_x)\n",
    "end = time\n",
    "elapsed = end - start\n",
    "```\n",
    "Using CUDA events, this becomes:\n",
    "\n",
    "```\n",
    "start = cuda.event() #create start event\n",
    "end = cuda.event()   #create end event\n",
    "start.record()       #read clock before execution\n",
    "sKernel[gridDims, blockDims](d_f, d_x) #call for kernel execution\n",
    "end.record()         #request clock read after execution\n",
    "end.synchronize()    #make sure execution is complete before reading clock\n",
    "elapsed = cuda.event_elapsed_time(start, end) #compute interval duration\n",
    "```\n",
    "\n",
    "The listing below of _map_parallel_event.py_ includes both `time()` statments and event timing so the results can be compared:\n",
    "\n",
    "```\n",
    "File: parallel_event.py\n",
    "01: import math\n",
    "02: import numpy as np\n",
    "03: from numba import jit, cuda, float32\n",
    "04: from time import time\n",
    "05: \n",
    "06: PI = np.pi\n",
    "07: TPB = 32\n",
    "08: \n",
    "09: @cuda.jit(device = True)\n",
    "10: def s(x0):\n",
    "11: \treturn (1.-2.*math.sin(PI*x0)**2)\n",
    "12: \n",
    "13: @cuda.jit #Lazy compilation\n",
    "14: #@cuda.jit('void(float32[:], float32[:])') #Eager compilation\n",
    "15: def sKernel(d_f, d_x):\n",
    "16: \ti = cuda.grid(1)\n",
    "17: \tn = d_x.shape[0]\t\n",
    "18: \tif i < n:\n",
    "19: \t\td_f[i] = s(d_x[i]) #content of `for` loop in serial version\n",
    "20: \n",
    "21: def sArray(x):\n",
    "22: \tn = x.shape[0]\n",
    "23: \td_x = cuda.to_device(x)\n",
    "24: \td_f = cuda.device_array(n, dtype = np.float32) #need dtype spec for eager compilation\n",
    "25: \tblockDims = TPB\n",
    "26: \tgridDims = (n+TPB-1)//TPB\n",
    "27: \n",
    "28: \te_start = cuda.event()\n",
    "29: \te_end = cuda.event()\n",
    "30: \tstart = time()\n",
    "31: \te_start.record()\n",
    "32: \tsKernel[gridDims, blockDims](d_f, d_x)\n",
    "33: \tend = time()\n",
    "34: \te_end.record()\n",
    "35: \te_end.synchronize()\n",
    "36: \tevent_time = cuda.event_elapsed_time(e_start, e_end)\n",
    "37: \telapsed_time = end -  start\n",
    "38: \tprint(\"--- Kernel time(): %3.4f milliseconds ---\" % (1000*elapsed_time))\n",
    "39: \tprint(\"--- Kernel event: %3.4f milliseconds ---\" % (event_time))\n",
    "40: \treturn d_f.copy_to_host()\n",
    "```\n",
    "\n",
    "$$ \\text{Listing 3.2 - } map\\_parallel\\_event.py$$\n",
    "\n",
    "Let's look at the output when we run the _map_ app with the previous print statements commented out and with `sArray` imported from _map/parallel_event.py_. For both kernel executions (and ahead-of-time compilation), the interval measured by `time()` is again $0$ ms which is not valid. This is simply telling us that the _kernel launch_ takes less than the millisecond precision of `time()`. Againg for both kernel executions, the timing measured by CUDA events is resolved to last for about $0.4$ s.\n",
    "\n",
    "```\n",
    "--- Kernel time(): 0.0000 milliseconds ---\n",
    "--- Kernel event: 0.4250 milliseconds ---\n",
    "--- Kernel time(): 0.0000 milliseconds ---\n",
    "--- Kernel event: 0.4246 milliseconds ---\n",
    "```\n",
    "Again, let's inspect the results when we increase the array size by an order of magnitude:\n",
    "```\n",
    "--- Kernel time(): 0.0000 milliseconds ---\n",
    "--- Kernel event: 4.1481 milliseconds ---\n",
    "--- Kernel time(): 0.0000 milliseconds ---\n",
    "--- Kernel event: 4.1497 milliseconds ---\n",
    "```\n",
    "\n",
    "Again, `time()` fails to capture the kernel execution time while the event-based timings appropriately increase by about an order of magnitude. The moral of the story is to you can use python's `time()` for most executions on the host, __be sure to use CUDA events for timing asynchronous operations such as kernel execution__.\n",
    "\n",
    "> Note that omitting the `end.synchronize()` before the call of `cuda.event_elapsed_time(start, end)` produces the error `CudaAPIError: [600] Call to cuEventElapsedTime results in CUDA_ERROR_NOT_READY`. If you encounter this error, check that you are properly synchronized before computing the time interval.\n",
    "\n",
    "\n",
    "## 3.2  Profiling\n",
    "\n",
    "In addition to tools for basic timing measurement, CUDA provides other tools for obtaining information about code performance (including timings). at this point, we will avoid a major tangent into the details of profiling, but a quick mention of profiling tools is in order. \n",
    "\n",
    "In previous versions, CUDA offered a list of tools including the following:\n",
    "\n",
    "- `cuda-memcheck` to check for illegal memory access and memory leaks.\n",
    "- NVIDIA Profiler, `nvprof`, offering command line access to a variety of performance data. (Unfortunately, this tool was not available for python under Windows.)\n",
    "- NVIDIA Visual Profiler, `nvvp`, offering a graphical user interface (GUI) for accessing performance data and execution timelines.\n",
    "- NVIDIA NSight which integrated the performance measurement tools into integrated development environments (IDEs) such as Visual Studio and Eclipse.\n",
    "\n",
    "The bad news is that these tools are being deprecated (which means that there is still some availability, but they will be removed in an upcoming release of a new version of CUDA which typically happens at least once a year). The good news is that they are being replaced with a new set of tools including:\n",
    "\n",
    "- NSight Systems, the new \"first stop\" profiling tool.\n",
    "- NSight Compute which focuses on performance of compute kernels.\n",
    "\n",
    "These tools are so new that there is not a lot of information out yet about putting them to work. However, they do include at least a temporary fix for accessing the capabilities of  `nvprof` from the command-line interface (CLI) even under Windows. The essential command is found in NSight Compute documentation under \"NSight Compute CLI Quickstart\" at:\n",
    "\n",
    "https://docs.nvidia.com/nsight-compute/NsightComputeCli/index.html#quick-start\n",
    "\n",
    "There it tells you to open a terminal, navigate to the directory where your code (let's call it `main.py`) resides, and run the following command:\n",
    "\n",
    "```\n",
    "nv-nsight-cu-cli -o profile python main_timed.py\n",
    "```\n",
    "\n",
    "This runs the NSight CUDA Command Line Interface (under the alias `nv-nsight-cu-cli`) on the code executed by calling `python main.py` and save the generated data to a file named `profile` (with file extension `.nsight-cuprof-report`). Sample output to the terminal looks like the following:\n",
    "\n",
    "```\n",
    "==PROF== Connected to process 21804 (C:\\Users\\storti\\anaconda3\\python.exe)\n",
    "==PROF== Profiling \"sKernel$241\" - 1: 0%....50%....100% - 24 passes\n",
    "==PROF== Profiling \"sKernel$241\" - 2: 0%....50%....100% - 24 passes\n",
    "==PROF== Disconnected from process 21804\n",
    "==PROF== Report: profile.nsight-cuprof-report\n",
    "```\n",
    "\n",
    "The output lines start with `==PROF==` are generated by the profiler and indicate that:\n",
    "\n",
    "- The profiler connects to the launched compute process.\n",
    "- The kernel \"sKernel$241\" is identified for profiling.\n",
    "- Performance data is collected over multiple execution passes.\n",
    "- The profiler disconnects from the process.\n",
    "- The performance data is written to the output file.\n",
    "  \n",
    "The performance data collected can be viewed by opening the report file using NSight Compute (which should be available in the NVIDIA folder created when you installed the CUDA Toolkit.)\n",
    "\n",
    "![](nsight-compute.png)\n",
    "\n",
    "$$ \\text{Fig. 3.1 - Sample view of profile using NSight Compute.}$$\n",
    "\n",
    "It would be an understatement to note that a _lot_ of information is presented here in a rather dense format. However, we can pick out a few items of interest. In 2 places (just to the right of \"Current\" next to the blue square near the upper left corner and at the top-right corner of the \"Speed of Light\" section), we can see that the average time for kernel execution is $423.36 \\mu s$ which is quite close to the result of the event-based timing. Also in the \"Speed of Light\" section the blue bars show that we are achieving about 90% of the theoretical maximum computing throughput but only using about 30% of the theoretical maximum data transfer rate.\n",
    "\n",
    " > This kernel is said to be \"compute-bound\" because it bumps up against the limit of computing throughput. Kernels with lower compute rating and high data transfer \"Speed of Light\" ratings are called \"memory-bound\".\n",
    "\n",
    "As we deal with more sophisticated computations, we will return for more detailed look at relevant profiling data. In the meantime, you can also test for memory access erros by running `cuda-memcheck python main_timed.py`. If you make your grid size not an even multiple of the block size and delete the `if i < n:` bounds test in the kernel, then `cuda-memcheck` will produce quite a bit of output including the following indications of attempts to access out-of-bounds array entries and copy results back to an out-of-bounds index in `f`:\n",
    "\n",
    "```\n",
    "========= CUDA-MEMCHECK\n",
    ".\n",
    ".\n",
    ".\n",
    "========= Invalid __global__ read of size 4\n",
    "=========     at 0x00000158 in cudapy::parallel_event::sKernel$241(Array<float, int=1, A, mutable, aligned>, Array<float, int=1, A, mutable, aligned>)\n",
    "=========     by thread (31,0,0) in block (200,0,0)\n",
    "=========     Address 0x70120c8f8 is out of bounds\n",
    ".\n",
    ".\n",
    ".\n",
    "========= Program hit CUDA_ERROR_LAUNCH_FAILED (error 719) due to \"unspecified launch failure\" on CUDA API call to cuMemcpyDtoH_v2.\n",
    ".\n",
    ".\n",
    ".\n",
    "========= ERROR SUMMARY: 2 errors\n",
    "```\n",
    "\n",
    "## 3.3 Debugging\n",
    "\n",
    "Depending on your code development environment of choice, you may have access to a reasonable set of debugging tools (setting break points, inspecting variable and array values, identifying error sources, etc.) for regular python code, but full-featured debugging of SIMT parallel code can be tough to come by. \n",
    "\n",
    "> If you use Visual Studio or Eclipse, you can get an NSight plug-in that incorporates tools for dubugging kernel code into your IDE. If you have access to such tools, you are encouraged to use them. Here we aim to provide a workable alternative for when fully integrated debugging of kernel code is not available.\n",
    "\n",
    "Fortunately, numba provides a middle-ground approach based on __CUDA Simulation Mode__.\n",
    "In simulation mode, the code runs entirely on the CPU but with the CPU simulating what the GPU will do when the code is run in parallel (give or take a few restrictions described in the numba docs). There is one overriding takeaway:\n",
    "\n",
    "__Simulation mode gives you a way to fully develop and debug parallel code without needing access to a system with CUDA-enabled hardware.__ If you are working on a system without a suitable graphics card, you can still write and debug code so that it is ready to run when you get access to CUDA-enabled hardware elsewhere. \n",
    "\n",
    "___If you are dependent on cloud-based CUDA hardware, you should get in the habit of developing and debugging in simulation mode on your own system, and then run your code on the coud-based server when it is known to be ready for parallel execution.___\n",
    "\n",
    "The mechanics of entering simulation mode involves setting the value for an environment variable, so you will need to figure out how to do that on your system. Here is one example of how it is done. Using Visual Studio Code in Windows, executing any python code (or selecting \"New Terminal\" from the \"Terminal\" menu) opens a PowerShell terminal window. In PowerShell, environment variables are set using the following syntax:\n",
    "\n",
    "`$env:VAR_NAME=\"VALUE\"`\n",
    "\n",
    "In this case, numba specifies the environment variable name to be `NUMBA_ENABLE_CUDASIM`, and the value is set to 1 for simulation mode and 0 otherwise; so the environment variable to enable simulation mode is set by running the following command in PowerShell:\n",
    "\n",
    "`$env:NUMBA_ENABLE_CUDASIM=\"1\"`\n",
    "\n",
    "The PowerShell command for exiting simulation mode is:\n",
    "\n",
    "`$env:NUMBA_ENABLE_CUDASIM=\"0\"`\n",
    "\n",
    "The PowerShell command for checking the value of the environment variable is:\n",
    "\n",
    "`$env:NUMBA_ENABLE_CUDASIM`\n",
    "\n",
    "> The simulator is \"smart\" enough to simulate much of the standard functionality of a generic CUDA-capable GPU, but it is not able to simulate the characteristics and limitations (such as available memory or compute capibility) of a specific GPU. Tests of such properties will need to be run on a CUDA-enabled system.\n",
    "\n",
    "Note that simulation mode involves a small number of cores simulating the work of a large number of cores. As a result, the ___simulation computation times may be considerably longer___ so be sure to develop code on small example problems. When you move to a CUDA-enabled system, test that the small problem sill works as expected, then increase the problem size and take advantage of the power of large-scale parallelism.\n",
    "\n",
    "Simulation mode involves an inherent tradeoff: a significant reduction in performance is involved but, in return, access is gained to debugging tools that can be applied even in parallel kernel code. The debugger is `pdb` (for \"Python DeBugger\"), and some basics are provided in the numba docs. For specifics of using `pdb` to debug kernel code, see the section \"Debugging CUDA Python code\" at https://numba.pydata.org/numba-doc/dev/cuda/simulator.html and the `pdb` docs at https://docs.python.org/2/library/pdb.html\n",
    "\n",
    "Kernel code involves a computational grid typically with numerous blocks and threads and, under the SIMT model, all the threads in a warp are executing the same operations in lockstep. Both of these considerations suggest that it would be redundant and potentially overwhelming to try to debug every thread. A more productive approach is to pick a particular thread in a particular block, and inspect that single execution of the kernel code. \n",
    "\n",
    "Here is example code for implementing that plan. Suppose that we decide to focus on thread with index `T = 2` in the block with index `B = 1`, we create the equivalent of a breakpoint at the line of interest (typically near the start of the kernel) by inserting the following code:\n",
    "\n",
    "```\n",
    "T,B = 2,1\n",
    "if threadIdx.x == T and blockIdx.x == B:\n",
    "    breakpoint()\n",
    "```\n",
    "Calling `breakpoint()` imports `pdb` and starts an execution trace. The `if` statement ensures that this only occurs during execution of the specified thread.\n",
    "\n",
    "> You may also see an \"old school\" way to import 'pbd', set a breakpoint, and initiate an execution trace:\n",
    "```\n",
    "T,B = 2,1\n",
    "if threadIdx.x == T and blockIdx.x == B:\n",
    "        from pdb import set_trace; set_trace()\n",
    "```\n",
    ">Here the debugger is initiated by calling `set_trace()` which is imported from `pdb`.\n",
    "\n",
    "\n",
    "\n",
    "When `breakpoint()` is called, the debugger starts to run as indicated by the terminal prompt changing to `(Pdb)`. With the debugger running, you can do enter terminal commands (terminated by hitting the `<Enter>` key) to perform a variety of operations including:\n",
    "\n",
    "- Inspect the value of a variable by entering its name.\n",
    "- Inspect the value of an expression involving variable names.\n",
    "- Step to the next line of execution with `s` or `step` or `n` or `next`. Step means go to the next line in any function, while next means go to the next line in the current function (so a function called on that line is executed in its entirety rather than line-by-line.)\n",
    "- Continue to the next break point with `c`.\n",
    "- Continue execution to line #L with `j L` or `jump L`.\n",
    "- Print the arguments of a function call with `a` or `args`.\n",
    "  \n",
    "Below is the listing for a code that computes the element-wise sum of 2 input arrays. The operation is parallelized by calling `vec_add_kernel` to which the snippet has been added to debug thread 2 in block 1.\n",
    "\n",
    "```\n",
    "File: vec_add_gdb.py\n",
    "01: from numba import cuda\n",
    "02: import numpy as np\n",
    "03: #import pdb\n",
    "04: \n",
    "05: N = 128\n",
    "06: TPB = 32\n",
    "07: BPG = (N+TPB-1)//TPB\n",
    "08: T = 2 #thread index for debug\n",
    "09: B = 1 #block index for debug\n",
    "10: \n",
    "11: @cuda.jit(debug=True)\n",
    "12: def vec_add_kernel(out, u, v):\n",
    "13:     x = cuda.threadIdx.x\n",
    "14:     bx = cuda.blockIdx.x\n",
    "15:     bdx = cuda.blockDim.x \n",
    "16:     if x == T and bx == B:\n",
    "17:         breakpoint()\n",
    "18:     i = cuda.grid(1)\n",
    "19:     j = bx * bdx + x\n",
    "20:     out[i] = u[i] + v[i]\n",
    "21:     diff = j-i\n",
    "22: \n",
    "23: def vec_add(u,v):\n",
    "24:     n = u.shape[0]\n",
    "25:     d_u = cuda.to_device(u)\n",
    "26:     d_v = cuda.to_device(v)\n",
    "27:     d_out = cuda.device_array(N)\n",
    "28:     vec_add_kernel[BPG,TPB](d_out, d_u, d_v)\n",
    "29:     return d_out.copy_to_host()\n",
    "30: \n",
    "31: u = np.ones(N)\n",
    "32: v = np.ones(N)\n",
    "33: C = vec_add(u,v)\n",
    "34: #print(C)\n",
    "```\n",
    "\n",
    "$$\\text{Listing 3.3 - } vec\\_add\\_gdb.py$$\n",
    "\n",
    "Here is a sample output of a debugging session. `(Pdb)` is the prompt for input, and the remainder of those lines are debugging input commands. Lines starting with `#` have been inserted to describe the ensuing operation. Lines with no \"prefix\" are outputs from the debugger, and a summary of common `pdb` commands is given in Table 3.1 below.\n",
    "\n",
    "```\n",
    "> C:\\path_info...\\vec_add_gdb.py(22)vec_add_kernel()\n",
    "#First line in terminal indicates location of breakpoint\n",
    "#Second line indicates line awaiting execution at break\n",
    "-> i = cuda.grid(1)\n",
    "#print value of 'x'\n",
    "(Pdb) x\n",
    "2\n",
    "#print value of ''bx'\n",
    "(Pdb) bx\n",
    "1\n",
    "#print value of 'i'\n",
    "(Pdb) i\n",
    "*** NameError: name 'i' is not defined\n",
    "#execute next command where 'i' gets assigned a value\n",
    "(Pdb) n\n",
    "-> j = bx * bdx + x\n",
    "#print value of 'i'\n",
    "(Pdb) i\n",
    "34\n",
    "#evaluate expression for value to be assigned to 'j'\n",
    "(Pdb) bx * bdx + x\n",
    "34\n",
    "#evaluate next line where 'j' gets assigned a value\n",
    "(Pdb) n\n",
    "-> out[i] = u[i] + v[i]\n",
    "#print value of 'j'\n",
    "(Pdb) j\n",
    "*** The 'jump' command requires a line number\n",
    "#'j' is shorthand for 'jump' so use 'p' to 'print'\n",
    "(Pdb) p j\n",
    "34\n",
    "#evaluate expression to be assigned to 'j'\n",
    "(Pdb) u[i] + v[i]\n",
    "2.0\n",
    "#print value of 'out[i]'\n",
    "(Pdb) out[i]\n",
    "nan\n",
    "#not yet assigned so execute next line\n",
    "(Pdb) n\n",
    "-> diff = j-i\n",
    "#print value of 'out[i]'\n",
    "(Pdb) out[i]\n",
    "2.0\n",
    "#quit debugger\n",
    "(Pdb) q\n",
    "```\n",
    "\n",
    "| Command\t| Key\t| Description |\n",
    "|---------|-----|-------------|\n",
    "| Next\t| n\t| Execute next line |\n",
    "| Step\t|s |\tStep into a subroutine |\n",
    "| Print\t<v_> | p <v_> |\tPrint value of variable <v_> |\n",
    "| <v_> | | Print value of variable <v_> |\n",
    "| <expr_> | | Evaluate and print expression |\n",
    "|Return\t|r\t|Run until the current subroutine returns |\n",
    "|Continue\t| c |\tStop debug; continue execution |\n",
    "|Quit\t|q |\tQuit pdb  |\n",
    "\n",
    "$$ \\text{Table 3.1 - } pdb \\text{ commands} $$\n",
    "\n",
    "While this use of `pdb` may not be quite as slick as a debugger completely integrated into an IDE with a full GUI, it does provide a tool for inspecting the details of SIMT code and ironing out issues that arise when writing kernels. Getting some practice using `pdb` is a good idea and can really pay off as you progress to writing more complicated kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}