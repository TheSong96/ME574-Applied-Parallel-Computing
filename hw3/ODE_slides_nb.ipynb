{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Finite Differences and Numerical Solution of Differential Equations\n",
    "\n",
    "We started with difference equations:\n",
    " - logistic map\n",
    " - Mandelbrot set\n",
    "  \n",
    "Shift to numerical solution of differential equations. \n",
    "\n",
    "- Use finite differences based on  __Taylor series__\n",
    "- Convert ODEs to difference equations\n",
    "\n",
    "## Numerical Differentiation\n",
    "\n",
    "Differentiation (and integration): basic operations of calculus\n",
    "\n",
    "Definition of the derivative:<br>\n",
    "\n",
    "$$\\frac{d f(t)}{dt} = \\lim_{\\Delta t\\to 0} \\frac{f(t+\\Delta t)-f(t)}{\\Delta t}$$ (1)\n",
    "\n",
    "<br>Digital computation $\\implies$ Floating-point numbers\n",
    "\n",
    "Floating-point numbers $\\implies$ finite set of finite numbers\n",
    "\n",
    "No infinitesimals $\\implies$ nothing actually $\\to 0$\n",
    "\n",
    "Feasible alternative: compute approx. of limit $\\implies$ Taylor series\n",
    "\n",
    "\n",
    "$$f(t+\\Delta t) = f(t)+\\Delta t\\frac{df(t)}{dt}+\\frac{\\Delta t^2}{2!} \\frac{d^2f(t)}{dt^2}+\\frac{\\Delta t^3}{3!} \\frac{d^3f(c_1)}{dt^3}$$ (2)\n",
    "\n",
    "<br>\n",
    "\n",
    "Solving for $\\frac{df(t)}{dt}$ gives __finite difference__ formula:\n",
    "<br>\n",
    "\n",
    "$$\\frac{df(t)}{dt} = \\frac{f(t + \\Delta t) - f(t)}{\\Delta t} + O(\\Delta t)$$ (3)\n",
    "\n",
    "<br>\n",
    "\n",
    "_General terminology_: __Finite difference:__ Approx. of derivative in terms of function evaluations at finitely separated points.\n",
    "\n",
    "_Particular case:_ __$1^{st}$-order forward diff. approx. of $1^{st}$ derivative__: \n",
    "- __First-order:__ Error term proportional to $(\\Delta t)^1$ \n",
    "- __Forward difference:__ Looks forward (evaluates at $t$ and $t + \\Delta t$).\n",
    "\n",
    "Or look backward ($\\Delta t \\to -\\Delta t$) $\\implies$ alternative Taylor expansion:\n",
    "<br>\n",
    "$$f(t-\\Delta t) = f(t)-\\Delta t\\frac{df(t)}{dt}+\\frac{\\Delta t^2}{2!} \\frac{d^2f(t)}{dt^2}-\\frac{\\Delta t^3}{3!} \\frac{d^3f(c_1)}{dt^3}$$ (4)\n",
    "<br>\n",
    "Compare to:\n",
    "\n",
    "$$f(t+\\Delta t) = f(t)+\\Delta t\\frac{df(t)}{dt}+\\frac{\\Delta t^2}{2!} \\frac{d^2f(t)}{dt^2}+\\frac{\\Delta t^3}{3!} \\frac{d^3f(c_1)}{dt^3}$$ (2)\n",
    "\n",
    "> Quick aside on symmetry: \n",
    "<br>Eq.2 $\\pm$ Eq.4 $\\implies$ odd/even terms cancel\n",
    "<br>But first, solve Eq.4 for $\\frac{df(t)}{dt}\\ldots$\n",
    "\n",
    "$\\implies$ First-order backward difference formula: \n",
    "<br>\n",
    "\n",
    "$$\\frac{df(t)}{dt} = \\frac{f(t) - f(t - \\Delta t)}{\\Delta t} + O(\\Delta t)$$ (5)\n",
    "<br>\n",
    "For higher-order estimate, subtract Eq.4 from Eq.2:  \n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "$$f(t+\\Delta t) - f(t-\\Delta t) = 2 \\Delta t\\frac{df(t)}{dt}+2 \\frac{\\Delta t^3}{3!} \\frac{d^3f(c_1)}{dt^3}$$ (6)\n",
    "\n",
    "<br>\n",
    "Even terms cancel, including the degree 2 term that led to the leading order error in the previous formula  \n",
    "<br> <br>\n",
    "\n",
    "\n",
    "Solve for $\\frac{df(t)}{dt}$ $\\implies$ __2nd order central difference formula__: \n",
    "\n",
    "$$\\frac{df(t)}{dt} = \\frac{f(t + \\Delta t) - f(t - \\Delta t)}{2 \\Delta t} + O(\\Delta t)^2$$ (7)\n",
    "\n",
    "<br>\n",
    "\n",
    "More evaluation points (+Taylor series) $\\implies$ families of finite difference approximations of various derivatives with various orders of accuracy. (See tables...)\n",
    "\n",
    "An important formula: <br>\n",
    "__Second-order central difference estimate of the $2^{nd}$ derivative__\n",
    "\n",
    "1. Sum the Taylor series (Eqs. 2&4) so the first derivatives cancel \n",
    "\n",
    "2. Solve for the second derivative:\n",
    "\n",
    "$$\\frac{d^2f(t)}{dt^2} = \\frac{f(t + \\Delta t) -2 f(t) + f(t - \\Delta t)}{(\\Delta t)^2} + O(\\Delta t)^2$$ (7)\n",
    "\n",
    "<br>\n",
    "\n",
    "- Means to compute floating-point approx. derivatives\n",
    "  \n",
    "- Basis in Taylor series $\\implies$ order of the __truncation error__\n",
    "- Estimate  of how error depends on the spacing $\\Delta t = h$\n",
    "- Useful formulas involve truncation errors proportional to some positive power $n$; i.e. $\\propto(h)^n$. \n",
    "- Expect decreasing spacing $\\implies$ decreasing error.\n",
    "\n",
    "Optimal accuracy is obtained by choosing $\\Delta t$ as small as possible?\n",
    "\n",
    "__NO!__ Why? \n",
    "\n",
    "2 major sources of error in digital computing:\n",
    "- Truncation\n",
    "  - Neglecting higher order terms in series expansion\n",
    "- __Roundoff__\n",
    "  - error incurred in approximating an infinite-precision real number by a finite-precision floating point number. \n",
    "- As $h$ becomes very small, the evaluation points get so closer than floating-point system cannot accurately resolve.\n",
    "\n",
    "Typical case:\n",
    "- For large spacing:\n",
    "  - truncation error dominates\n",
    "  - error can be reduced by decreasing the spacing. \n",
    "- For small spacing:\n",
    "  - roundoff error dominates\n",
    "  - smaller spacing makes the error worse, not better.\n",
    "\n",
    "__Bottom line on spacing and error:__ \n",
    "- Truncation error dominates for large spacing and gets better as spacing decreases. \n",
    "- Roundoff error dominates for small spacing and gets better as spacing increases. \n",
    "- Some \"middle ground\" spacing minimizes total error.\n",
    "- Typically some small integer root of _machine epsilon_    \n",
    "  - Largest number that can be added to 1 and produce 1 as the floating-point result\n",
    "  - Measure of bound on relative precision\n",
    "\n",
    "Ready to move on to differential equations.\n",
    "\n",
    "\n",
    "\n",
    "## Ordinary Differential Equations (ODEs) - Initial Value Problems (IVPs)\n",
    "\n",
    "Intro to nomenclature: \n",
    "\n",
    "- __Differential equation__: equation with one or more derivatives. \n",
    "- __Independent variables__ appear on the bottom of a derivative.\n",
    "- __Dependent variables__ appear on the top of a derivative.\n",
    "- __Ordinary differential equation__, or system of equations, (ODEs) have a single independent variable. \n",
    "- __PArtial differential equations (PDEs)__ have multiple independent variables. \n",
    "- The __order__ of the DEs is the order of the highest derivative.\n",
    "- __Linear__ refers to dependent variable:\n",
    "\n",
    "  - $m y'' + c y' + k y = sin(\\omega t)$ is __linear__ in y\n",
    "\n",
    "  - $m y'' + c y' + k sin(y) = A sin(\\omega t)$ is __nonlinear__ in y\n",
    "\n",
    "Start with system of 1 or more $1^{st}$-order ODEs \n",
    "\n",
    "> Note that there is a standard \"trick\" to convert an $n^{th}$-order ODE to a system of first-order ODEs: Simply introduce new __dependent__ variables for derivatives of order $0$ through $n-1$. This immediately creates a system of $n$ first-order ODEs; the first $n-1$ equations define the variables, and the original ODE rewritten in terms of the new variables becomes the $n^{th}$ equation. <br>$m y'' + c y' + k sin(y) = A sin(\\omega t)$ becomes \n",
    "> $$ \n",
    "\\begin{aligned}\n",
    "u_0' &= u_1 \\\\\n",
    "m u_1' &= -c u_1 -k sin(u_0) + A sin(\\omega t)\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Euler's method\n",
    "\n",
    "Most fundamental problem: compute approx. solution of a single $1^{st}$-order ODE with a specified initial value:\n",
    "\n",
    "$$\\frac{dy}{dt} = f(t,y)\\;, \\qquad y(0) = y_0$$ (8)\n",
    "\n",
    "You have likely seen the simplest approach: __Euler's method__:\n",
    "\n",
    "- Classic \"stepping\" or \"marching\" method\n",
    "- Replace continuous independent variable $t$ (\"time\") with discrete version:\n",
    "$$t_n = t_0 + n \\Delta t = t_0 + n h$$ (9)\n",
    "\n",
    "- Compute sequence of values at the discrete \"future\" times based on the existing known \"history\": \n",
    "\n",
    "$$y_n = y(t_n) = y(t_0 + n h)$$ (10)\n",
    "\n",
    "- End up again with a difference equation (or map)\n",
    "- Derive difference equation by replacing the derivative with a finite difference estimate.\n",
    "- Euler's method uses simple forward difference estimator\n",
    "\n",
    "$$\\frac{dy}{dt} = f(t,y)\\;, \\qquad y(0) = y_0 \\rightarrow \\frac{y_{n+1} - y_n}{h}\\approx f(t_n, y_n)$$ (11)\n",
    "\n",
    "- Solve for $y_{n+1}$ to obtain formula for  value at the next time step\n",
    "\n",
    "$$y_{n+1} = y_n + h f(t_n, y_n)$$ (12)\n",
    "\n",
    "_ More precisely called the __forward Euler method__ (uses forward difference estimate for the $1^{st}$ derivative) \n",
    "- Also called __explicit Euler method__: Eq.12 gives explicit formula for $y_{n+1}$ given $y_n, t_n,f$.\n",
    "\n",
    "- Computes rate of change at the beginning of a time step, $f(y_n,t_n)$ and applies it over full step (from $t_n$ to $t_{n+1}$)\n",
    "- Ignores the change of rate over short time step, which is not exact $\\iff$ truncation error. \n",
    "  - Truncation error for each Euler step of Euler's $\\sim  O(\\Delta t)$ \n",
    "  - Number of time steps to cover an interval $O(\\Delta t)^{-1} \\implies$ __global truncation error__ $\\sim O(1)$. \n",
    "  - Euler's method is simple & sometimes gives useful  results, but do not expect smaller $h$ to give better results.\n",
    "\n",
    "- \"Refinable\" results requires higher-order truncation error.\n",
    "  - How to create higher-order methods?\n",
    "  - More terms in Taylor series + more evaluation points.\n",
    "\n",
    "__Modified Euler-Cauchy Method__\n",
    "\n",
    "Next step uses Euler's method to estimate an intermediate point where rate of change (i.e. the function $f$) is computed that gives better estimate of what happens over the interval. \n",
    "\n",
    "Euler-Cauchy steps:\n",
    "\n",
    "1) Compute derivate (RHS) at initial time (left side of interval)\n",
    "\n",
    "$$rate_{left} = f(t, y(t))$$ (13)\n",
    "\n",
    "2) Use that derivative value to compute \"Euler\" estimate of midpoint value.\n",
    "\n",
    "$$y_{mid} = y(t) + \\frac{h}{2} rate_{left}$$ (14)\n",
    "\n",
    "3) Use midpoint value to compute midpoint rate estimate.\n",
    "\n",
    "$$rate_{mid} = f(t+\\frac{h}{2}, y_{mid})$$ (15)\n",
    "\n",
    "4) Use the midpoint rate estimage to compute the next value:\n",
    "$$y_{RK2}(t+h) = y(t) + h (rate_{mid})$$ (16)\n",
    "\n",
    "Put the pieces together:\n",
    "$$y_{RK2}(t+h) = y(t)+h f\\big(t+\\frac{h}{2}, y(t)+\\frac{h}{2} \\; f(t,y(t)) \\big)$$ (17)\n",
    "\n",
    "- E-C truncation error:\n",
    "  - $2^{nd}$-order local error \n",
    "  - $1^{st}$-order global error \n",
    "  - Smaller steps reduces global error (for \"friendly\" ODEs).\n",
    "\n",
    "Why does modified Euler-Cauchy formula have subscript \"RK2\"?\n",
    "\n",
    "Euler's method and modified E-C are first 2 methods in a family called __Runge-Kutta methods__:\n",
    "\n",
    "To achieve higher order methods in RK family:\n",
    "- Compute more rate estimates\n",
    "- Combine results to knock out higher powers of $h$ in T.S.\n",
    "- Increase degree of leading neglected term\n",
    "\n",
    "## $4^{th}$ Order Runge-Kutta Method\n",
    "\n",
    "Most common version: __Fourth-Order Runge_Kutta (RK4)__\n",
    "- Computes average of 4 rate estimates \n",
    "  - 1 at the start of the interval\n",
    "  - 2 in the middle\n",
    "  - 1 at the end)\n",
    "\n",
    "Details of RK4 algorithm:\n",
    "\n",
    "1) Compute initial rate\n",
    "$$f_1 = f(t_n,y_n)$$ (18)\n",
    "\n",
    "1) Use initial rate to estimate midpoint values\n",
    "$$f_2 = f(t_n+\\frac{h}{2}, y_n+\\frac{h}{2}f_1)$$ (19)\n",
    "\n",
    "3) Use the midpoint estimate to compute improved midpoint estimate\n",
    "$$f_3 = f(t_n+\\frac{h}{2}, y_n+\\frac{h}{2}f_2)$$ (20)\n",
    "\n",
    "4) Use improved midpoint estimate to compute right-side estimate\n",
    "$$f_4 = f(t_n+h,y_n+h f_3)$$ (21)\n",
    "\n",
    "5) Compute weighted sum  of contributions to cancel as many terms in the Taylor series as possible.\n",
    "6) \"RK4\" formula: local error $\\sim O(h^5)$, global error $\\sim O(h^4)$:\n",
    "$$y_{n+1} = y_n + \\frac{h}{6} [f_1 + 2 f_2 +2 f_3 + f_4]$$ (22)\n",
    "\n",
    "## Application to Stability Analysis\n",
    "\n",
    "When studying the behavior of dynamical systems, there are a few common phases of the analysis. \n",
    "\n",
    "- Modeling and application of physical principles. If you are studying a pendulum, you might make modeling approximations such as:\n",
    "\n",
    "1) The pendulum is a rigid body.\n",
    "\n",
    "2) The pendulum remains in a vertical plane.\n",
    "\n",
    "3) Friction may or may not be considered according to some manageable model.\n",
    "\n",
    "4) Some exterior forcing may be applied.\n",
    "\n",
    "- Based on modeling assumptions, apply Newton's laws or Lagrange's equations to obtain equations of motion: (typically) $2^{nd}$-order ODE governing the motion of the system. \n",
    "\n",
    "Simplest model (ignoring damping and forcing) would be:\n",
    "\n",
    "$$\\theta'' = -sin(\\theta); \\qquad \\theta(0)=\\theta_0, theta'(0) = \\theta'_0$$ (23)\n",
    "\n",
    "Convert to $1^{st}$-order system:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "u_0' &= u_1 \\\\\n",
    "u_1' &= -sin(u_0)\n",
    "\\end{aligned}$$ (24)\n",
    "\n",
    "with initial conditions (ICs:) $u_0(0)=\\theta_0$, $u_1(0)=\\theta'_0$.\n",
    "\n",
    "Coding up the algorithms above, you now have tools to compute an approximate numerical solution for a particular choice of ICs.\n",
    "\n",
    "Next stage of the dynamic analysis typically involves identification of equilibrium/steady-state solutions. \n",
    "- Set time derivatives to 0 (so all rates vanish)\n",
    "- Solve rate equations for equilibrium values. \n",
    "- In this case, the equilibrium equations are:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "0 &= u[1] \\\\\n",
    "0 &= -sin(u[0])\n",
    "\\end{aligned}$$ (25)\n",
    "\n",
    "$\\implies$ 2 distinct equilibria:\n",
    "- $u_1=0$ (so the pendulum is at rest) \n",
    "- Equilibrium positions:\n",
    "  - $u_0 = 0$ (straight down beneath the pivot)\n",
    "  - $u_0 = \\pi$ (straight up above the pivot)\n",
    "- Both valid equilibrium solutions\n",
    "  - One is routinely observed\n",
    "  - The other does not occur in practice\n",
    "- The distiction is __stability__.\n",
    "\n",
    "Many definitions of stability exist, but a simple one works here:\n",
    "\n",
    " __Stable equilibrium__ has a surrounding neighborhood where all ICs lead to trajectories that remain near the equilibrium.\n",
    "\n",
    "If __any__ ICs in the neighborhood produce a trajectory that leaves the neighborhood, the equilibrium is __unstable__.\n",
    "\n",
    " ___Stability is not about particular set of ICs; it is about the collective behavior of ALL the initial conditions in the vicinity of the equilibrium.___\n",
    "\n",
    "$\\implies$ Stability is an ideal application for parallelization. \n",
    "\n",
    "- Computation of trajectory/history for a particular set of ICs must be computed serially\n",
    "  - Cannot reasonably compute $y((k+1) h)$ without knowing $y(k h))\n",
    "  - But trajectory for one set of ICs can be computed completely independent of the trajectory for any other set of ICs. \n",
    "- Stability studies lead naturally to the idea of launching a computational grid with kernel that solves the ODE for particular Ics. \n",
    "- GPU-based parallel computation can solve concurrently for MANY (realistically, millions of) ICs around the equilibrium.\n",
    "- 3D plot of something like the ratio of the final and initial distance from the equilibrium will serve as  stability indicator (with potential \"false positives\").\n",
    "\n",
    "Pursue this concept in more detail in an upcoming homework...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}