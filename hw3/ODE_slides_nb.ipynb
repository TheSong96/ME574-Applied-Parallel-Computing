{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Finite Differences and Numerical Solution of Differential Equations\n",
    "\n",
    "We started with difference equations:\n",
    " - logistic map\n",
    " - Mandelbrot set\n",
    "  \n",
    "Shift to numerical solution of differential equations. \n",
    "\n",
    "- Use finite differences based on  __Taylor series__\n",
    "- Convert ODEs to difference equations\n",
    "\n",
    "## Numerical Differentiation\n",
    "\n",
    "Differentiation (and integration): basic operations of calculus\n",
    "\n",
    "Definition of the derivative:<br>\n",
    "\n",
    "$$\\frac{d f(t)}{dt} = \\lim_{\\Delta t\\to 0} \\frac{f(t+\\Delta t)-f(t)}{\\Delta t}$$ (1)\n",
    "\n",
    "<br>Digital computation $\\implies$ Floating-point numbers\n",
    "\n",
    "Floating-point numbers $\\implies$ finite set of finite numbers\n",
    "\n",
    "No infinitesimals $\\implies$ nothing actually $\\to 0$\n",
    "\n",
    "Feasible alternative: compute approx. of limit $\\implies$ Taylor series\n",
    "\n",
    "\n",
    "$$f(t+\\Delta t) = f(t)+\\Delta t\\frac{df(t)}{dt}+\\frac{\\Delta t^2}{2!} \\frac{d^2f(t)}{dt^2}+\\frac{\\Delta t^3}{3!} \\frac{d^3f(c_1)}{dt^3}$$ (2)\n",
    "\n",
    "<br>\n",
    "\n",
    "Solving for $\\frac{df(t)}{dt}$ gives __finite difference__ formula:\n",
    "<br>\n",
    "\n",
    "$$\\frac{df(t)}{dt} = \\frac{f(t + \\Delta t) - f(t)}{\\Delta t} + O(\\Delta t)$$ (3)\n",
    "\n",
    "<br>\n",
    "\n",
    "_General terminology_: __Finite difference:__ Approx. of derivative in terms of function evaluations at finitely separated points.\n",
    "\n",
    "_Particular case:_ __$1^{st}$-order forward diff. approx. of $1^{st}$ derivative__: \n",
    "- __First-order:__ Error term proportional to $(\\Delta t)^1$ \n",
    "- __Forward difference:__ Looks forward (evaluates at $t$ and $t + \\Delta t$).\n",
    "\n",
    "Or look backward ($\\Delta t \\to -\\Delta t$) $\\implies$ alternative Taylor expansion:\n",
    "<br>\n",
    "$$f(t-\\Delta t) = f(t)-\\Delta t\\frac{df(t)}{dt}+\\frac{\\Delta t^2}{2!} \\frac{d^2f(t)}{dt^2}-\\frac{\\Delta t^3}{3!} \\frac{d^3f(c_1)}{dt^3}$$ (4)\n",
    "<br>\n",
    "Compare to:\n",
    "\n",
    "$$f(t+\\Delta t) = f(t)+\\Delta t\\frac{df(t)}{dt}+\\frac{\\Delta t^2}{2!} \\frac{d^2f(t)}{dt^2}+\\frac{\\Delta t^3}{3!} \\frac{d^3f(c_1)}{dt^3}$$ (2)\n",
    "\n",
    "> Quick aside on symmetry: \n",
    "<br>Eq.2 $\\pm$ Eq.4 $\\implies$ odd/even terms cancel\n",
    "<br>But first, solve Eq.4 for $\\frac{df(t)}{dt}\\ldots$\n",
    "\n",
    "$\\implies$ First-order backward difference formula: \n",
    "<br>\n",
    "\n",
    "$$\\frac{df(t)}{dt} = \\frac{f(t) - f(t - \\Delta t)}{\\Delta t} + O(\\Delta t)$$ (5)\n",
    "<br>\n",
    "For higher-order estimate, subtract Eq.4 from Eq.2:  \n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "$$f(t+\\Delta t) - f(t-\\Delta t) = 2 \\Delta t\\frac{df(t)}{dt}+2 \\frac{\\Delta t^3}{3!} \\frac{d^3f(c_1)}{dt^3}$$ (6)\n",
    "\n",
    "<br>\n",
    "Even terms cancel, including the degree 2 term that led to the leading order error in the previous formula  \n",
    "<br> <br>\n",
    "\n",
    "\n",
    "Solve for $\\frac{df(t)}{dt}$ $\\implies$ __2nd order central difference formula__: \n",
    "\n",
    "$$\\frac{df(t)}{dt} = \\frac{f(t + \\Delta t) - f(t - \\Delta t)}{2 \\Delta t} + O(\\Delta t)^2$$ (7)\n",
    "\n",
    "<br>\n",
    "\n",
    "More evaluation points (+Taylor series) $\\implies$ families of finite difference approximations of various derivatives with various orders of accuracy. (See tables...)\n",
    "\n",
    "An important formula: <br>\n",
    "__Second-order central difference estimate of the $2^{nd}$ derivative__\n",
    "\n",
    "1. Sum the Taylor series (Eqs. 2&4) so the first derivatives cancel \n",
    "\n",
    "2. Solve for the second derivative:\n",
    "\n",
    "$$\\frac{d^2f(t)}{dt^2} = \\frac{f(t + \\Delta t) -2 f(t) + f(t - \\Delta t)}{(\\Delta t)^2} + O(\\Delta t)^2$$ (7)\n",
    "\n",
    "<br>\n",
    "\n",
    "- Means to compute floating-point approx. derivatives\n",
    "  \n",
    "- Basis in Taylor series $\\implies$ order of the __truncation error__\n",
    "- Estimate  of how error depends on the spacing $\\Delta t = h$\n",
    "- Useful formulas involve truncation errors proportional to some positive power $n$; i.e. $\\propto(h)^n$. \n",
    "- Expect decreasing spacing $\\implies$ decreasing error.\n",
    "\n",
    "Optimal accuracy is obtained by choosing $\\Delta t$ as small as possible?\n",
    "\n",
    "__NO!__ Why? \n",
    "\n",
    "2 major sources of error in digital computing:\n",
    "- Truncation\n",
    "  - Neglecting higher order terms in series expansion\n",
    "- __Roundoff__\n",
    "  - error incurred in approximating an infinite-precision real number by a finite-precision floating point number. \n",
    "- As $h$ becomes very small, the evaluation points get so closer than floating-point system cannot accurately resolve.\n",
    "\n",
    "Typical case:\n",
    "- For large spacing:\n",
    "  - truncation error dominates\n",
    "  - error can be reduced by decreasing the spacing. \n",
    "- For small spacing:\n",
    "  - roundoff error dominates\n",
    "  - smaller spacing makes the error worse, not better.\n",
    "\n",
    "__Bottom line on spacing and error:__ \n",
    "- Truncation error dominates for large spacing and gets better as spacing decreases. \n",
    "- Roundoff error dominates for small spacing and gets better as spacing increases. \n",
    "- Some \"middle ground\" spacing minimizes total error.\n",
    "- Typically some small integer root of _machine epsilon_    \n",
    "  - Largest number that can be added to 1 and produce 1 as the floating-point result\n",
    "  - Measure of bound on relative precision\n",
    "\n",
    "Ready to move on to differential equations.\n",
    "\n",
    "\n",
    "\n",
    "## Ordinary Differential Equations (ODEs) - Initial Value Problems (IVPs)\n",
    "\n",
    "Intro to nomenclature: \n",
    "\n",
    "- __Differential equation__: equation with one or more derivatives. \n",
    "- __Independent variables__ appear on the bottom of a derivative.\n",
    "- __Dependent variables__ appear on the top of a derivative.\n",
    "- __Ordinary differential equation__, or system of equations, (ODEs) have a single independent variable. \n",
    "- __PArtial differential equations (PDEs)__ have multiple independent variables. \n",
    "- The __order__ of the DEs is the order of the highest derivative.\n",
    "- __Linear__ refers to dependent variable:\n",
    "\n",
    "  - $m y'' + c y' + k y = sin(\\omega t)$ is __linear__ in y\n",
    "\n",
    "  - $m y'' + c y' + k sin(y) = A sin(\\omega t)$ is __nonlinear__ in y\n",
    "\n",
    "Start with system of 1 or more $1^{st}$-order ODEs \n",
    "\n",
    "> Note that there is a standard \"trick\" to convert an $n^{th}$-order ODE to a system of first-order ODEs: Simply introduce new __dependent__ variables for derivatives of order $0$ through $n-1$. This immediately creates a system of $n$ first-order ODEs; the first $n-1$ equations define the variables, and the original ODE rewritten in terms of the new variables becomes the $n^{th}$ equation. <br>$m y'' + c y' + k sin(y) = A sin(\\omega t)$ becomes \n",
    "> $$ \n",
    "\\begin{aligned}\n",
    "u_0' &= u_1 \\\\\n",
    "m u_1' &= -c u_1 -k sin(u_0) + A sin(\\omega t)\n",
    "\\end{aligned}$$\n",
    "\n",
    "### Euler's method\n",
    "\n",
    "Most fundamental problem: compute approx. solution of a single $1^{st}$-order ODE with a specified initial value:\n",
    "\n",
    "$$\\frac{dy}{dt} = f(t,y)\\;, \\qquad y(0) = y_0$$ (8)\n",
    "\n",
    "You have likely seen the simplest approach: __Euler's method__:\n",
    "\n",
    "- Classic \"stepping\" or \"marching\" method\n",
    "- Replace continuous independent variable $t$ (\"time\") with discrete version:\n",
    "$$t_n = t_0 + n \\Delta t = t_0 + n h$$ (9)\n",
    "\n",
    "- Compute sequence of values at the discrete \"future\" times based on the existing known \"history\": \n",
    "\n",
    "$$y_n = y(t_n) = y(t_0 + n h)$$ (10)\n",
    "\n",
    "- End up again with a difference equation (or map)\n",
    "- Derive difference equation by replacing the derivative with a finite difference estimate.\n",
    "- Euler's method uses simple forward difference estimator\n",
    "\n",
    "$$\\frac{dy}{dt} = f(t,y)\\;, \\qquad y(0) = y_0 \\rightarrow \\frac{y_{n+1} - y_n}{h}\\approx f(t_n, y_n)$$ (11)\n",
    "\n",
    "- Solve for $y_{n+1}$ to obtain formula for  value at the next time step\n",
    "\n",
    "$$y_{n+1} = y_n + h f(t_n, y_n)$$ (12)\n",
    "\n",
    "_ More precisely called the __forward Euler method__ (uses forward difference estimate for the $1^{st}$ derivative) \n",
    "- Also called __explicit Euler method__: Eq.12 gives explicit formula for $y_{n+1}$ given $y_n, t_n,f$.\n",
    "\n",
    "- Computes rate of change at the beginning of a time step, $f(y_n,t_n)$ and applies it over full step (from $t_n$ to $t_{n+1}$)\n",
    "- Ignores the change of rate over short time step, which is not exact $\\iff$ truncation error. \n",
    "  - Truncation error for each Euler step of Euler's $\\sim  O(\\Delta t)$ \n",
    "  - Number of time steps to cover an interval $O(\\Delta t)^{-1} \\implies$ __global truncation error__ $\\sim O(1)$. \n",
    "  - Euler's method is simple & sometimes gives useful  results, but do not expect smaller $h$ to give better results.\n",
    "\n",
    "- \"Refinable\" results requires higher-order truncation error.\n",
    "  - How to create higher-order methods?\n",
    "  - More terms in Taylor series + more evaluation points.\n",
    "\n",
    "__Modified Euler-Cauchy Method__\n",
    "\n",
    "Next step uses Euler's method to estimate an intermediate point where rate of change (i.e. the function $f$) is computed that gives better estimate of what happens over the interval. \n",
    "\n",
    "Euler-Cauchy steps:\n",
    "\n",
    "1) Compute derivate (RHS) at initial time (left side of interval)\n",
    "\n",
    "$$rate_{left} = f(t, y(t))$$ (13)\n",
    "\n",
    "2) Use that derivative value to compute \"Euler\" estimate of midpoint value.\n",
    "\n",
    "$$y_{mid} = y(t) + \\frac{h}{2} rate_{left}$$ (14)\n",
    "\n",
    "3) Use midpoint value to compute midpoint rate estimate.\n",
    "\n",
    "$$rate_{mid} = f(t+\\frac{h}{2}, y_{mid})$$ (15)\n",
    "\n",
    "4) Use the midpoint rate estimage to compute the next value:\n",
    "$$y_{RK2}(t+h) = y(t) + h (rate_{mid})$$ (16)\n",
    "\n",
    "Put the pieces together:\n",
    "$$y_{RK2}(t+h) = y(t)+h f\\big(t+\\frac{h}{2}, y(t)+\\frac{h}{2} \\; f(t,y(t)) \\big)$$ (17)\n",
    "\n",
    "- E-C truncation error:\n",
    "  - $2^{nd}$-order local error \n",
    "  - $1^{st}$-order global error \n",
    "  - Smaller steps reduces global error (for \"friendly\" ODEs).\n",
    "\n",
    "Why does modified Euler-Cauchy formula have subscript \"RK2\"?\n",
    "\n",
    "Euler's method and modified E-C are first 2 methods in a family called __Runge-Kutta methods__:\n",
    "\n",
    "To achieve higher order methods in RK family:\n",
    "- Compute more rate estimates\n",
    "- Combine results to knock out higher powers of $h$ in T.S.\n",
    "- Increase degree of leading neglected term\n",
    "\n",
    "## $4^{th}$ Order Runge-Kutta Method\n",
    "\n",
    "Most common version: __Fourth-Order Runge_Kutta (RK4)__\n",
    "- Computes average of 4 rate estimates \n",
    "  - 1 at the start of the interval\n",
    "  - 2 in the middle\n",
    "  - 1 at the end)\n",
    "\n",
    "Details of RK4 algorithm:\n",
    "\n",
    "1) Compute initial rate\n",
    "$$f_1 = f(t_n,y_n)$$ (18)\n",
    "\n",
    "1) Use initial rate to estimate midpoint values\n",
    "$$f_2 = f(t_n+\\frac{h}{2}, y_n+\\frac{h}{2}f_1)$$ (19)\n",
    "\n",
    "3) Use the midpoint estimate to compute improved midpoint estimate\n",
    "$$f_3 = f(t_n+\\frac{h}{2}, y_n+\\frac{h}{2}f_2)$$ (20)\n",
    "\n",
    "4) Use improved midpoint estimate to compute right-side estimate\n",
    "$$f_4 = f(t_n+h,y_n+h f_3)$$ (21)\n",
    "\n",
    "5) Compute weighted sum  of contributions to cancel as many terms in the Taylor series as possible.\n",
    "6) \"RK4\" formula: local error $\\sim O(h^5)$, global error $\\sim O(h^4)$:\n",
    "$$y_{n+1} = y_n + \\frac{h}{6} [f_1 + 2 f_2 +2 f_3 + f_4]$$ (22)\n",
    "\n",
    "## Application to Stability Analysis\n",
    "\n",
    "When studying the behavior of dynamical systems, there are a few common phases of the analysis. \n",
    "\n",
    "- Modeling and application of physical principles. If you are studying a pendulum, you might make modeling approximations such as:\n",
    "\n",
    "1) The pendulum is a rigid body.\n",
    "\n",
    "2) The pendulum remains in a vertical plane.\n",
    "\n",
    "3) Friction may or may not be considered according to some manageable model.\n",
    "\n",
    "4) Some exterior forcing may be applied.\n",
    "\n",
    "- Based on modeling assumptions, apply Newton's laws or Lagrange's equations to obtain equations of motion: (typically) $2^{nd}$-order ODE governing the motion of the system. \n",
    "\n",
    "Simplest model (ignoring damping and forcing) would be:\n",
    "\n",
    "$$\\theta'' = -sin(\\theta); \\qquad \\theta(0)=\\theta_0, theta'(0) = \\theta'_0$$ (23)\n",
    "\n",
    "Convert to $1^{st}$-order system:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "u_0' &= u_1 \\\\\n",
    "u_1' &= -sin(u_0)\n",
    "\\end{aligned}$$ (24)\n",
    "\n",
    "with initial conditions (ICs:) $u_0(0)=\\theta_0$, $u_1(0)=\\theta'_0$.\n",
    "\n",
    "Coding up the algorithms above, you now have tools to compute an approximate numerical solution for a particular choice of ICs.\n",
    "\n",
    "Next stage of the dynamic analysis typically involves identification of equilibrium/steady-state solutions. \n",
    "- Set time derivatives to 0 (so all rates vanish)\n",
    "- Solve rate equations for equilibrium values. \n",
    "- In this case, the equilibrium equations are:\n",
    "\n",
    "$$ \n",
    "\\begin{aligned}\n",
    "0 &= u[1] \\\\\n",
    "0 &= -sin(u[0])\n",
    "\\end{aligned}$$ (25)\n",
    "\n",
    "$\\implies$ 2 distinct equilibria:\n",
    "- $u_1=0$ (so the pendulum is at rest) \n",
    "- Equilibrium positions:\n",
    "  - $u_0 = 0$ (straight down beneath the pivot)\n",
    "  - $u_0 = \\pi$ (straight up above the pivot)\n",
    "- Both valid equilibrium solutions\n",
    "  - One is routinely observed\n",
    "  - The other does not occur in practice\n",
    "- The distiction is __stability__.\n",
    "\n",
    "Many definitions of stability exist, but a simple one works here:\n",
    "\n",
    " __Stable equilibrium__ has a surrounding neighborhood where all ICs lead to trajectories that remain near the equilibrium.\n",
    "\n",
    "If __any__ ICs in the neighborhood produce a trajectory that leaves the neighborhood, the equilibrium is __unstable__.\n",
    "\n",
    " ___Stability is not about particular set of ICs; it is about the collective behavior of ALL the initial conditions in the vicinity of the equilibrium.___\n",
    "\n",
    "$\\implies$ Stability is an ideal application for parallelization. \n",
    "\n",
    "- Computation of trajectory/history for a particular set of ICs must be computed serially\n",
    "  - Cannot reasonably compute $y((k+1) h)$ without knowing $y(k h))\n",
    "  - But trajectory for one set of ICs can be computed completely independent of the trajectory for any other set of ICs. \n",
    "- Stability studies lead naturally to the idea of launching a computational grid with kernel that solves the ODE for particular Ics. \n",
    "- GPU-based parallel computation can solve concurrently for MANY (realistically, millions of) ICs around the equilibrium.\n",
    "- 3D plot of something like the ratio of the final and initial distance from the equilibrium will serve as  stability indicator (with potential \"false positives\").\n",
    "\n",
    "Pursue this concept in more detail in an upcoming homework...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch. 4 Evaluating Functions on Multi-Dimensional Grids\n",
    "\n",
    "In the notebook for Chapter 2 \"Function Evaluations and the Map Pattern\" we created\n",
    "serial and parallel implementations of the _map_ app to evaluate\n",
    "a trigonometric function on a one-dimensional grid of input data.\n",
    "The notebook for Chapter 3 \"Tools for Timing, Profiling, and Debugging\" is in development and will come out later). In this notebook, we create the _map2D_ and _map3D_ apps\n",
    "that extend the map pattern to perform function evaluation on two-dimensional (2D)\n",
    "and three-dimensional (3D) grids of input data.\n",
    "\n",
    "> CUDA natively supports indexing up\n",
    "to three dimensions so we will typically not\n",
    "continue on to higher dimensional\n",
    "grids. If you need to handle a problem which would naturally involve a geometric grid of dimension $D_g>3$, you can include a loop over additional dimensions within the kernel or you can map the index values for the $D_g$-dimensional geometric grid to the index values of a $D_c$-dimensional computational grid with $D_c \\leq 3$.\n",
    "\n",
    "\n",
    "# 4.1 Evaluating a Two-Dimensional Array of Function Values: The _map2D_ App\n",
    "\n",
    "We start by using the _map_ app as a template to expand upon and create\n",
    "the _map2D_ app that computes values of a function on a 2D grid of input points. For the sample code, we choose a new function of 2 variables:\n",
    "\n",
    "$$f_{2D}(x,y) = \\frac{sin(\\pi x) sinh(\\pi y)}{sinh(\\pi)}$$ (4.1)\n",
    "\n",
    "which involves both trigonometric and hyperbolic functions.\n",
    "\n",
    "As alluded to in the comment above, there are really 2 grids to deal with: a geometric grid of points $\\{x_p,y_q\\}=\\{x_0+p \\Delta x, y_0 + q \\Delta y \\}$ where we want to evaluate the function $f(x_p,y_q)$ and a computational grid of threads with indices `i,j`. \n",
    "\n",
    "While there is considerable freedom in choosing the relation between the geometric and computational grids, here we present a simple, intuitive, and very commonly used choice. We establish a 1:1 correspondence between the geometric and computational grids, by identifying $p$ with `i` and $q$ with `j` so that each 2-tuple of index values specifies a corresponding point on the geometric grid. \n",
    "\n",
    "Having established the relationship of the computational grid indices to the coordinate values for points on the geometric grid, we are now ready to look at implementations that map the function of 2 variables $f(x,y)$ on the 2D geometric grid. \n",
    "\n",
    "# 4.1 Serial Implementation\n",
    "\n",
    "Just like the one-dimensional _map_ app, our serial implementation of _map2D_ consists of 2 files:\n",
    "\n",
    "1. _apps/map2D/main.py_ which contains the `main()` function that initializes a 2D array, calls a function to evaluate the function on the 2D geometric grid,\n",
    "and plots the computed 2D array of function values.\n",
    "\n",
    "2. _apps/map2D/serial.py_ which computes the function values, stores them in the appropriate positions in the array, and returns the array of computed function values.\n",
    "\n",
    "Listing 4.1 and 4.3 together form the implementation of _map2D_. In both cases, there are only a few changes needed to transform the previously created _map_ app into the _map2D_ app.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NX, NY = 128, 64\n",
    "\n",
    "def main():\n",
    "\tx = np.linspace(0, 1, NX)\n",
    "\ty = np.linspace(0, 1, NY)\n",
    "\n",
    "\tfrom serial import fArray2D\n",
    "\tf = fArray2D(x, y)\n",
    "\n",
    "\tX, Y = np.meshgrid(x, y)\n",
    "\tplt.contourf(X, Y, f.T) #`T` is shorthand for `transpose()`\n",
    "\tplt.xlabel(\"X\")\n",
    "\tplt.ylabel(\"Y\")\n",
    "\tplt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n",
    "```\n",
    "\n",
    " $$ \\text{ Listing 4.1: }apps/map2D/main.py$$\n",
    " \n",
    "> For applications like creating multidimensional plots, the numpy function `meshgrid()` is often used to create tuples of coordinate values corresponding to points on a cartesian grid from 1D arrays of coordinate values along each coordinate direction. Here `x` and `y` are the arrays of 1D coordinate values, so  `np.meshgrid(x,y)` constructs the 2-tuples of $\\{x,y\\}$ values on the 2D geometric grid. `X,Y = np.meshgrid(x,y)` assigns the components of the tuple to 1D arrays `X` and `Y` to produce 1D arrays of coordinates of points that arise while traversing the grid (which, together with the 2D array of function values, correspond to the positional arguments expected for standard 2D plotting functions like `contourf()`.)\n",
    "\n",
    "Most of the differences between _apps/map/main.py_ and  _apps/map2D/main.py_ arise in defining additional variables for the added dimension. Listing 4.2 provides a comparison of the files with code from _apps/map/main/py_ appearing as comments after `#`. `#SAME` indicates that the same line appears in both files.\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NX, NY = 128, 64 # N = 128\n",
    "\n",
    "def main():\n",
    "\tx = np.linspace(0, 1, NX) #SAME\n",
    "\ty = np.linspace(0, 1, NY)\n",
    "\n",
    "\tfrom serial import fArray2D #SAME\n",
    "\tf = fArray2D(x, y) #f = sArray(x)\n",
    "\n",
    "\tX, Y = np.meshgrid(x, y) #plot.plot(x,f,'bo')\n",
    "\tplt.contourf(X, Y, f.T) #`T` is shorthand for `transpose()`\n",
    "\tplt.xlabel(\"X\")\n",
    "\tplt.ylabel(\"Y\")\n",
    "\tplt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n",
    "```\n",
    "\n",
    "$$ \\text{Listing 4.2: Comparison of } apps/map2D/main.py \\text{ and } apps/map/main.py $$\n",
    "\n",
    "Instead of a single value, `N` , to define the array size, we use two separate values, `NX` and `NY` , to specify the number of points in each grid direction.\n",
    "`np.linspace()` is called a second time to create an array `y` of coordinate values along the $y$-axis. The name of the imported function is changed to `fArray2D()`. Lastly, The graphical output is a contour plot so the commands\n",
    "to create the plot requires the additional line calling `np.meshgrid()` to create the grid of $x,y$ coordinate pairs, and `plt.plot()` is replaced with `plt.contourf(X, Y, f.T)`.\n",
    "\n",
    "> The output array is transposed using `f.T` as shortthand for f.transpose()` to provide data in the format required by `contourf()`.\n",
    "\n",
    "The file _apps/map2D/serial.py_ defining the imported function is changed in much the same way to produce _apps/\n",
    "map2D/main.py_ as shown in Listing 4.3\n",
    "\n",
    "```\n",
    "1 import math\n",
    "2 import numpy as np\n",
    "3 PI = np.pi\n",
    "4 def f2D(x, y):\n",
    "5   return math.sin( PI *x)* math.sinh( PI *y)/ math.sinh( PI )\n",
    "6\n",
    "7 def fArray2D(x, y):\n",
    "8   nx = x.size\n",
    "9   ny = y.size\n",
    "10  f = np.empty((nx ,ny), dtype = np.float32)\n",
    "11\n",
    "12  for i in range(nx):\n",
    "13      for j in range(ny):\n",
    "14          f[i,j] = f2D(x[i], y[j])\n",
    "15  return f\n",
    "```\n",
    "$$\\text{Listing 4.3: } apps/map2D/serial.py$$\n",
    "\n",
    "All of the alterations are to accommodate the new dimension. On Lines 4-5, the function to be evaluated at each point is named `f2D()` and corresponds to Equation 4.1, $f_{2D}(x, y)$. Lines 8-9 assign to the variables `nx` and `ny` the sizes of the input coordinate arrays `x` and `y`. On line 10, an empty numpy array `f` of size `nx` $\\times$ `ny` is created to provide storage for the array of 32-bit function values to be computed. Lines 12-13 specify nested `for` loops that iterate over the range of index values corresponding to the size of the input arrays. On line 14, the function `f2D()` is evaluated with arguments corresponding to the coordinates of the corresponding point on the geometric grid (stored in `x[i],y[j]`), and the function value is stored as the corresponding array element `f[i,j]`.\n",
    "\n",
    "That concludes the serial implementation, and we are ready to parallelize.\n",
    "\n",
    "### 4.1.2 Parallel Implementation\n",
    "\n",
    "As with the 1D case, the plan for parallelization is straightforward. Modify the imported function `fArray2D()` so that the `for` loops in the serial implementation are replaced with the launch of a kernel function `fKernel2D[]()` that performs the computation previously contained in the loops.\n",
    "\n",
    "We store the modified code in _apps/map2D/parallel.py_ which is shown in Listing 4.4. The changes performed to get from _apps/map2D/serial.py_ to  _apps/map2D/parallel.py_ are described below, and you should compare listing 4.4 with Listing 4.3 as you read that description.\n",
    "\n",
    "```\n",
    "1 import math\n",
    "2 import numpy as np\n",
    "3 from numba import cuda\n",
    "4 PI = np.pi\n",
    "5 TPBX = 16\n",
    "6 TPBY = 16\n",
    "7\n",
    "8 @cuda.jit( device = True )\n",
    "9 def f2D(x, y):\n",
    "10  return math.sin( PI *x)* math.sinh( PI *y)/ math.sinh( PI )\n",
    "11\n",
    "12 @cuda.jit('void (f4[:] , f4[:] , f4[: ,:])')\n",
    "13 def fKernel2D(d_x , d_y , d_f):\n",
    "14  i , j = cuda.grid(2)\n",
    "15  nx , ny = d_f.shape\n",
    "16  if i < nx and j < ny:\n",
    "17      d_f[i,j] = f2D(d_x[i], d_y[j])\n",
    "18\n",
    "19 def fArray2D(x, y):\n",
    "20  nx = x.size\n",
    "21  ny = y.size\n",
    "22\n",
    "23  d_x = cuda.to_device(x)\n",
    "24  d_y = cuda.to_device(y)\n",
    "25  d_f = cuda.device_array((nx , ny), dtype = np.float32)\n",
    "26\n",
    "27  gridDims = ((nx + TPBX - 1)//TPBX ,\n",
    "28              (ny + TPBY - 1)// TPBY )\n",
    "29  blockDims = (TPBX , TPBY )\n",
    "30\n",
    "31  fKernel2D[ gridDims , blockDims ](d_x , d_y , d_f)\n",
    "32\n",
    "33  return d_f.copy_to_host()\n",
    "```\n",
    "\n",
    "$$ \\text{Listing 4.4: } apps/map2D/parallel.py$$\n",
    "\n",
    "Like the serial version, the file begins by importing relevant packages, now including numba's `cuda` package to provide support for parallelism. Lines 4-5 assign values for the variables, `TPBX` and `TPBY`, that are used to define the size (number of threads along each index direction) of the blocks to be established when the kernel is launched.\n",
    "\n",
    "> __Execution parameter limitations:__ In the implementation of CUDA, a number of design decisions had to be made that lead to practical limitations on how it can be used. We have mentioned one already: Grids of dimension higher than 3 are not supported. Here we run into another important limit: There is a maximum number of threads that are allowed in a block, and the limit is 1024 for almost all currently available GPUs. Since $64 \\times 64 = 4096$, a $64 \\times 64$ block would violate the restriction and generate an error, so the sample code launches a grid with $16 \\times 16$ blocks (each with 256 threads). See the CUDA documentation (or Wikipedia's CUDA page) for the full set of technical specifications and limitations.\n",
    "\n",
    "Lines 9-10 define the function `f2d` that is to be evaluated at each grid point. As in the 1D case, this function will be called from the kernel, so the decorator `@cuda.jit( device = True )` is included on line 8, immediately before the function definition to identify it as a device function.\n",
    "\n",
    "The definition of the kernel function `fKernel2D()` appears on lines 12-17 including the decorator `@cuda.jit('void(f4[:] , f4[:] , f4[: ,:])')` that accomplishes several goals:\n",
    "\n",
    "1. The decorator identifies `fKernel2D` as a kernel function to be launched from the host and executed on the device.\n",
    "2. It satisfies the requirement that a kernel cannot return a value by specifying the return type `void`.\n",
    "3. The code in the parentheses follow `void` indicate that there will be 3 arguments, two 1D arrays (with a single colon in the square brackets) followed by a 2D array (with `[]:,:]`). `f4` indicates that 4 bytes of memory are allocate for each entry in the array. Thus, `f4` provides an abbreviated alternative to `np.float32`.\n",
    "\n",
    "The definition statement on line 13 incidates that `fKernel2D` takes 3 arguments: the device array `d_x` that should store a copy of the $x$-coordinates,  the device array `d_y` that should store a copy of the $y$-coordinates, and the device array `d_f` to store the computed function values. On line 14, `i,j = cuda.grid(2)`,  defines the 2-tuple of index values. \n",
    "\n",
    "> Note that `i,j = cuda.grid(2)` is numba's handy abbreviation for the equivalent code:\n",
    "<br>`i = threadIdx.x + blockDim.x * blockIdx.x`\n",
    "<br>`j = threadIdx.y + blockDim.y * blockIdx.y`\n",
    "<br>the first line of which defines `i` according to the  1D index formula, while the second line defines `j` simply by changing the \"suffix\" from `.x` to `.y`.\n",
    "\n",
    "Line 15 assigns to `nx,ny` the dimensions of the array `d_f` to be computed. Line 16 tests whether the indices `i` and `j` are within the index bounds of `d_f` and, if so, `d_f[i,j] = f2D(d_x[i], d_y[j])` evaluates the function at the corresponding point on the geometric grid and stores the result as the indexed entry in `d_f`.\n",
    "\n",
    "> __Bounds checking:__ If an array dimension is not an exact multiple of the corresponding block size, then the \"last\" block (with the largest `blockIdx` value) will include index values that lie beyond the extent of the array. To avoid reading or writing into unintended portions of memory, the kernel should routinely include a bounds check similar to the code on line 16. _Later we will consider whether this produces a performance penalty due to_ ___thread divergence___.\n",
    "\n",
    "The remainder of the file, lines 19-33, defines the wrapper function `fArray2D()`. Lines 20-21 assign to `nx` and `ny` the length of the input arrays of coordinate values `x` and `y`. Lines 23-24  call `cuda.to_device()` to create `d_x` and `d_y` (as \"mirror\" copies of the inputs `x` and `y`) that give the kernel access to copies of the input data. On line 25, `d_f` is created to provide a `nx` $\\times$ `ny` device array to store the function values. Lines 27-29 set the values for the execution parameters. Here, both `gridDims` and `blockDims` are 2-tuples (for a 2D grid). The components of `blockDims` are assigned to match the specified values, `TPBX` and `TBPY`, and each dimension of `gridDims` is computed from the corresponding component of `blockDims` according to the same formula used in the 1D case. The call to execute the kernel launch appears on line 31: `fKernel2D[ gridDims , blockDims ](d_x , d_y , d_f)`. This call matches the format of the 1D version: kernel function name, followed by `[ gridDims , blockDims ]`, followed by parentheses containing the comma-separated list of arguments). Finally, on line 33, the array of computed vaues are copied back to the host by `return d_f.copy_to_host()` so the output is available for plotting on the CPU side.\n",
    "\n",
    "> __Device array methods:__ `d_f` was created as a device array object, so it comes with a variety of methods including `copy_to_host()` for copying data from device to host.\n",
    "\n",
    "> __Return value:__ Note carefully how the results get back to the host. A device array is defined in the wrapper function and included as a kernel argument. The kernel stores the output in the device array, but does ___NOT___ return anything to the host. It is the wrapper function, not the kernel itself, that returns the output to the host using `copy_to_host()`.\n",
    "\n",
    "That completes the description of _apps/map2D/parallel.py_, and all that remains is a minor change to _apps/map2D/main.py_. To run the parallelized version of the _map2D_ app, we want to use the parallel version of `fArray2D()` instead of the serial version used previously, so on line 12 of  _apps/map2D/main.py_, `from serial import fArray2D` should be replaced by `from parallel import fArray2D`.\n",
    "\n",
    "> __File locations:__ For the `import` statement to work as desired, the files to be imported (_apps/map2D/serial.py_ and _apps/map2D/parallel.py_) should be located in the same directory as _apps/map2D/main.py_. To import from other locations, directory location information must be provided for files from which is imported.\n",
    "\n",
    "\n",
    "\n",
    "## 4.2 Evaluating a Three-Dimensional Array of Function Values: The map3D App\n",
    "\n",
    "Having seen the implementation of _map2D_, which basically involved adding a second coordinate direction to _map_, the extension to compute function values on a 3D grid should seem relatively straightforward; it involves similarly adding a third coordinate direction. Let’s start by choosing a function with three parameters:\n",
    "\n",
    "$$f_{3D}(x,y,z) = \\frac{sinh(\\pi y)}{sinh(\\pi)} sin(\\pi x)  cos(\\pi z) $$\n",
    "\n",
    "Listings 4.5, 4.6, and 4.7 show the code for _apps/map3D/main.py_, _apps/map3D/serial.py__, and _apps/map3D/parallel.py_. For now, we settle for printing results to the terminal; we will return later to consider visualization of 3d grids of data.\n",
    "\n",
    "```\n",
    "File: main.py\n",
    "01: import numpy as np\n",
    "02: import matplotlib.pyplot as plt\n",
    "03: \n",
    "04: NX, NY, NZ = 8, 8, 16\n",
    "05: \n",
    "06: def main():\n",
    "07: \tx = np.linspace(0, 1, NX)\n",
    "08: \ty = np.linspace(0, 1, NY)\n",
    "09: \tz = np.linspace(0, 1, NZ)\n",
    "10: \n",
    "11: \tfrom serial import fArray3D\n",
    "12: \tf = fArray3D(x, y, z)\n",
    "13: \tprint(f)\n",
    "14: \n",
    "15: if __name__ == '__main__':\n",
    "16: \tmain()\n",
    "\n",
    "```\n",
    "\n",
    "$$ \\text{Listing 4.5: } apps/map3D/main.py$$\n",
    "\n",
    "Note that _apps/map3D/main.py_ is a bit simpler (without the plotting code) and that number of blocks in each direction is reduced so that the total number of threads in each block, $8 \\times 8 \\times 8 = 512$, is under the 1024 limit.\n",
    "\n",
    "At this point, the code will hopefully seem pretty readable, so you should read through it and then experiment using it to see how it works.\n",
    "\n",
    "```\n",
    "File: serial.py\n",
    "01: import math\n",
    "02: import numpy as np\n",
    "03: PI = np.pi\n",
    "04: def f3D(x0, y0, z0):\n",
    "05: \treturn math.sin(PI * x0) * math.cos(PI * z0) * math.sinh(PI * y0) / math.sinh(PI)\n",
    "06: \n",
    "07: def fArray3D(x, y, z):\n",
    "08: \tnx = x.shape[0]\n",
    "09: \tny = y.shape[0]\n",
    "10: \tnz = z.shape[0]\n",
    "11: \tf = np.empty(shape=[nx,ny,nz], dtype = np.float32)\n",
    "12: \tfor i in range(nx):\n",
    "13: \t\tfor j in range(ny):\n",
    "14: \t\t\tfor k in range(nz):\n",
    "15: \t\t\t\tf[i,j,k] = f3D(x[i], y[j], z[k])\n",
    "16: \treturn f\n",
    "```\n",
    "$$ \\text{Listing 4.6: } apps/map3D/serial.py$$\n",
    "\n",
    "\n",
    "```\n",
    "File: parallel.py\n",
    "01: import math\n",
    "02: import numpy as np\n",
    "03: from numba import jit, cuda, float32\n",
    "04: \n",
    "05: TPBX, TPBY, TPBZ = 8, 8, 8\n",
    "06: \n",
    "07: @cuda.jit(device = True)\n",
    "08: def f3D(x0, y0, z0):\n",
    "09: \treturn math.sin(np.pi * x0) * math.cos(np.pi * z0) * math.sinh(np.pi * y0) / math.sinh(np.pi)\n",
    "10: \n",
    "11: @cuda.jit\n",
    "12: def fKernel3D(d_f, d_x, d_y, d_z):\n",
    "13: \ti,j,k = cuda.grid(3)\n",
    "14: \tnx,ny,nz = d_f.shape\t\n",
    "15: \tif i < nx and j < ny and k < nz:\n",
    "16: \t\td_f[i,j,k] = f3D(d_x[i], d_y[j], d_z[k])\n",
    "17: \n",
    "18: def fArray3D(x, y, z):\n",
    "19: \tnx = x.shape[0]\n",
    "20: \tny = y.shape[0]\n",
    "21: \tnz = z.shape[0]\n",
    "22: \td_x = cuda.to_device(x)\n",
    "23: \td_y = cuda.to_device(y)\n",
    "24: \td_z = cuda.to_device(z)\n",
    "25: \td_f = cuda.device_array(shape = [nx,ny,nz], dtype = np.float32)\n",
    "26: \tgridDims = (nx+TPBX-1)//TPBX, (ny+TPBY-1)//TPBY, (nz+TPBZ-1)//TPBZ\n",
    "27: \tblockDims = TPBX, TPBY, TPBZ\n",
    "28: \tfKernel3D[gridDims, blockDims](d_f, d_x, d_y, d_z)\n",
    "29: \n",
    "30: \treturn d_f.copy_to_host()\n",
    "```\n",
    "$$ \\text{Listing 4.7: } apps/map3D/parallel.py$$\n",
    "\n",
    "\n",
    "The changes toget from_apps/map2D/serial.py_ to  _apps/map3D/serial.py_ include renaming and updating the functions `f3D()` and `fArray3D()`. Each has three inputs for the $x$, $y$, and $z$ directions. `fArray3D()` includes a triply nested loop on Lines 12-15 with indices `i`, `j`, and `k` with bounds determined from the shapes of the input arrays `x` , `y` and `z`. Each output is stored as a 32-bit float in the three-dimensional array `f`.\n",
    "\n",
    "The modifications to _apps/map3D/parallel.py_ are similar. The suffix on the function names is changed from 2D to 3D and each takes 3 arguments. The indices in the kernel are assigned using `i, j, k = cuda.grid(3)`, and the bounds are determined from the shape of the 3D output device array `d_f`. Bounds are checked in all three directions, and the computed function value is stored in the appropriate\n",
    "location in `d_f`.\n",
    "\n",
    "The wrapper function uses the newly introduced `TPBZ` to set the `.z`-component size of each block. The input arrays are copied to device arrays on lines 22-24, and a three-dimensional output device array\n",
    "is created on Line 25. The execution parameters, which are now both tuples of length 3, are specified on Lines 26-27, and the kernel call occurs on line 28. The wrapper once again ends by copying the results\n",
    "from the output device array to the host on Line 30.\n",
    "\n",
    "This finishes our discussion on setting up multi-dimensional grids. After reading carefullly through this notebook, you should be ready to do Homework 3 and then move on to further notebooks.\n",
    "\n",
    "## Suggested Projects\n",
    "\n",
    "1. Experiment with removing the data type specifications in _apps/map2D/main.py_. Do the serial and parallel results agree exactly without explicit dtype specification. Try removing the data type specifications from the signature as well.\n",
    "\n",
    "2. Time the execution of the serial and parallel implementations of `fArray2D()`. Characterize the acceleration due to parallelization for a range of array sizes.\n",
    "\n",
    "3. What is largest square block size for which you can execute `fArray2D()`? What CUDA limit do you run into? What error message is generated?\n",
    "when the requested block is too large?\n",
    "\n",
    "4. Add a signature to the `fKernel3D()` function decorator.\n",
    "\n",
    "5. Time the execution of the serial and parallel implementations of `fArray3D()`. Characterize the acceleration due to parallelization for a range of array sizes.\n",
    "\n",
    "6. What is largest cubic block size for which you can execute `fArray3D()`? What CUDA limit do you run into? What error message is generated when the requested block is too large?\n",
    "\n",
    "7. Experiment with execution parameter specifications that change the \"aspect ratio\" of your blocks (i.e. square vs. rectangular blocks). Can you detect any patterns about how aspect ratio changes affect kernel execution times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
